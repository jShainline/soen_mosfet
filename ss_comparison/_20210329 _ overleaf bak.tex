\documentclass[twocolumn]{article}

\usepackage{geometry}
\geometry{textwidth = 18cm,textheight = 24cm}

\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{outlines}
\usepackage[dvipsnames]{xcolor}
\usepackage{textcomp}
\usepackage{float}
\usepackage{ulem}

\usepackage{hyperref}
\hypersetup{
    citecolor = red,
    filecolor = green,
    urlcolor = orange,
    colorlinks = true, %set true if you want colored links
    linktoc = all,     %set to all if you want both sections and subsections linked
    linkcolor = blue,  %choose some color if you want links to stand out
}

%\title{Comparison of semiconductor and superconductor hardware for large-scale optoelectronic neural systems}
\title{\textcolor{OliveGreen}{Considerations for neuromorphic supercomputing in semiconducting and superconducting optoelectronic hardware}}
\author{Bryce A. Primavera and Jeffrey M. Shainline}
\date{March 2021}

\begin{document}

\maketitle
\begin{abstract}
Any large-scale neuromorphic system striving for complexity at the level of the human brain and beyond will need to be co-optimized for communication and computation. Such reasoning leads to the proposal for optoelectronic neuromorphic platforms that leverage the complementary properties of optics and electronics. Optical communication allows for direct connections between neurons, which removes bottlenecks associated with network traffic. Electronic computation allows for complex synaptic and neuronal functions. Starting from the hypothesis \textcolor{red}{(conjecture?)} that future large-scale neuromorphic systems will utilize integrated photonics and fiber optics for communication in conjunction with electronics for computation, we consider two possible paths towards achieving this vision. The first is a semiconductor platform based on analog CMOS circuits and waveguide-integrated photodiodes. The second is a superconducting approach that utilizes Josephson Junctions \sout{(JJs)} \textcolor{red}{(let's try to avoid introducing acronyms until the main text)} and waveguide-integrated superconducting \sout{nanowire} single-photon detectors \sout{(SNSPDs)}. We argue that these two systems can \textcolor{ForestGreen}{both} implement \sout{similar} \textcolor{ForestGreen}{adequate} synaptic and neuronal dynamics. With this established, the two platforms are analyzed from several viewpoints: power consumption, speed, area, \sout{available} memory\textcolor{ForestGreen}{/plasticity} \sout{technologies}, cooling, and \sout{feasibility of} fabrication. While both platforms require significant technological innovations to become viable, we reach some early conclusions about their limits and device performance metrics that will \sout{likely} need to be met for optoelectronic neuromorphic supercomputers to become a useful paradigm. Notably, it is found that the minimum possible energy used for communicating events is likely to be of the same order in both cases, but the superconducting approach dramatically lessens the optical power required of the integrated light-sources, which remain the most speculative element of both platforms. An ideal neuromorphic platform also possesses local memory integrated within synaptic circuits. While synaptic circuits with activity-based plasticity mechanisms have seen great progress in recent years, learning functionality is still left wanting. Superconducting electronics introduces new, but rather unexplored paths for integrated memory that appear promising for synaptic adaptation based on the same signals used elsewhere in the network for communication and computation. Lastly, the physical, system-level instantiation of massive optoelectronic networks is considered for both cases, including cooling systems, area considerations, and the importance and prospects of 3D integration. Superconducting systems appear more naturally suited for 3D integration due to room-temperature processing. While this study cannot determine which nascent platform will be superior in the future, it enumerates a list of technological advances that will be required if either approach is going to achieve its lofty ambitions \sout{and identifies major concerns}. \textcolor{ForestGreen}{Our intention is for the list of necessary advances to aid the} community \sout{to accurately} is assessing the progress of both platforms \sout{in future years} and to \sout{maintain a focus on systems that will be truly scalable} \textcolor{ForestGreen}{direct attention to unique aspects of optoelectronic systems that become significant in the unique context of large-scale neural systems}. 
\end{abstract}

\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\tableofcontents

\section{\label{sec:introduction}Introduction}

The foundations of cognition remain a great frontier of science, with potentially enormous ramifications for technology and society. A platform capable of simulating neural function on the scale of the brain and beyond could be a powerful tool in unravelling these great mysteries. Achieving that scale, however, will likely require new architectures and the development of novel hardware. A significant challenge will be to construct systems that enable efficient communication with low-latency amongst billions or trillions neurons. Optical communication appears well-matched to the task, as the lack of resistive and capacitive parasitics makes optical links more amenable to the high fan-out required than electrical interconnects \cite{shainline2018largest}. In the present work, we consider aspects of optoelectronic hardware that must be considered if brain-scale systems are to be achieved. 

To date, most neuromorphic systems are fabricated in standard CMOS processes and make use of a digital communication infrastructure called Address Event Representation (AER). AER uses an addressing system to time-multiplex spiking events from many neurons on to a small number of buses in order to circumvent the physical issues that limit CMOS circuits to only a handful of outputs. This framework has been very successful and neuromorphic chips targeting edge applications are beginning to be commercialized \cite{merolla2014million, davies2018loihi}. Additionally, their programmability and accessibililty make them valuable tools for testing testing hypotheses in the neuroscience and computer science communities. However, multiplexing inherently introduces trade-offs between connectivity and latency. It is thus unclear if AER, and even electronic communication more broadly, will be the optimal solution for future neural systems pushing the limits of cognition.

Optical communication, in contrast, may eliminate the need for multiplexing, allowing direct, physical connections between neurons. Unlike electrons, photons do not interact strongly with each other, allowing for low cross-talk and high fan-out. Additionally, waveguides and optical fibers enable low-loss communication up to the scale of meters - an advantage commonly exploited by long-distance communication systems that has been increasingly applied to shorter and shorter length scales \cite{miller2017attojoule}. While the lack of interaction between photons is a benefit for communication, it is a detriment to computation. Electronic circuits are better suited to implement complicated, nonlinear neuronal functions. This has led to the hypothesis that the largest cognitive systems will be optoelectronic - utilizing optics for communication and electronics for computation \cite{shainline2018largest}.

A number of concepts for optoelectronic neuromorphic hardware have been proposed. In this paper, we will restrict ourselves to a discussion of only two classes of networks (superconducting and semiconducting) designed around three basic assumptions:

%A variety of promising optoelectronic neuromorphic platforms have been proposed (\cite{shainline2019superconducting, nazirzadeh2018energy}), but here we limit the scope of the present study to systems that adhere to the following design rules:

\begin{enumerate}
    \item Direct connections are superior to shared connections.
    \item Optical communication should be binary.
    \item All synaptic, dendritic, and somatic computations should be performed by electronic circuits.
\end{enumerate}

\begin{figure*}
    \centering
    \includegraphics[scale=1]{_Schematic.pdf}
    \caption{An abstract schematic of the class of optoelectronic neurons meeting the three criteria given above. Each synapse ($S_e$ and $S_i$ for expiatory and inhibitory synapses respectively) is implemented with a physical circuit block containing a detector and a temporal filter. The detector produces an all-or-nothing electrical pulse upon receipt of an optical spike which is then processed by the filter. The parameters of the filter (time constant, weight, etc.) can be set individually for each synapse. A local weight update circuit (W) implements plasticity mechanisms at each synapse. Synaptic outputs are integrated in the soma (grey) which drives an optical transmitter to downstream connections upon reaching threshold.}
    \label{fig:Schematic}
\end{figure*}

These three conditions follow from the principles already mentioned. Optical communication is attractive precisely because it eliminates the need for shared communication lines by allowing for neurons with high fan-out. This means there will be no performance bottlenecks associated with network traffic (latency is independent of network activity) and no overhead necessary to manage a massive address space (communication events do not require memory access). Additionally, when separate synapses are used for each connection, those synapses can be constructed with a wide variety of different properties. The second constraint concerning binary optical communication stems from the hypothesis that computation is best done in the electronic domain. This also minimizes the amount of optical energy necessary per spiking event. If electrical to optical conversion is costly, then binary optical communication will be the most efficient solution. Finally, implementing non-linear functions optically can be difficult, so it seems likely that the majority of computation should be done in the electronic domain.

With these general postulates established, a picture of the ideal hardware begins to emerge. There is a single optical transmitter at each neuron. This light emitter produces a short pulse of light each time the neuron spikes. The optical pulse is coupled into a waveguide and optical power is tapped off the waveguide for each downstream synapse. Each synapse contains a photodetector which registers an all or nothing spiking event. From there, all dendritic, weighting, summing, thresholding, and plasticity mechanisms are implemented in the electronic domain. A schematic of this general framework is shown in figure \ref{fig:Schematic}.

Following this model, a hardware platform known as SOENs (Superconducting OptoElectronic Networks) was proposed in 2016 \cite{shainline2017superconducting}. SOENs exploits several exciting physical devices that are only possible at low temperatures. Superconducting Nanowire Single Photon Detectors (SNSPDs) are efficient, low-power detectors that allow the optical pulses reaching each synapse to be as faint as a few photons. Superconducting Josephson Junctions (JJs) provide an electronic circuit element that can compactly implement a wide variety neuronal computations. Lastly, the low temperature operation also provides benefits for light sources, perhaps even making silicon light sources a viable option. An integrated silicon light source would make the challenge of massive, wafer-level optoelectronic systems a much more realistic endeavour. 

For all the possible advantages, it is worth considering whether all of these exotic superconducting devices are truly superior to a more conventional semiconducting approach to optoelectronic neuromorphic hardware. The spirit of the hardware could potentially be preserved with a one-to-one replacement of each superconducting device with its semiconductor analogue. Photodiodes could substitute for SNSPDs. MOSFETs could play the role of JJs in implementing neuronal computation. Intriguingly, photodiodes and MOSFETs have none of the cryogenic constraints of their superconducting counterparts. Semiconductor systems can be envisioned that operate at 4K, 77K, or even room-temperature. This approach also clearly benefits from decades of development that have made the semiconductor industry what it is today.

This paper seeks to analyze the suitability of semiconductor and superconductor platforms for implementing large-scale optoelectronic neuromorphic networks. Despite limiting our discussion only to architectures meeting our three assumptions, there is still a vast space of design choices, making it difficult to draw hard-and-fast conclusions. Nevertheless, some interesting results can be arrived at by identifying and analyzing the limits of technologies most likely to be used in each platform. Important benchmarks for device performance are also identified, which will hopefully be of use in monitoring the development of this field in the coming years.

\section{\label{sec:communication}Communication}
\subsection{Optical Receivers}
The receiver is a natural place to begin analyzing the power consumption of an optical interconnect. \textcolor{red}{(Why?)} There are two ways that the receiver influences the power budget of an optical link: (1) The receiver (and the electrical components it must drive) sets the minimum optical signal that needs to be produced by the light source and (2) the receiver may require electrical power of its own to run. In the following sections, we compare SOENs receivers \cite{shainline2019superconducting} to those most likely to be implemented in all-semiconductor approach to optoelectronic neuromorphic computing.

\subsubsection{SOENs Receivers}
As stated previously, the SOENs platform utilizes Superconducting Nanowire Single Photon Detectors (SNSPDs) to detect optical signals as low as a single photon. Physically, an SNSPD is simply a superconducting nanowire biased with a current source ($I_{spd} \approx 1\mu A$). The simple structure makes fabrication and waveguide integration straightforward. Photons travelling through a waveguide evanescently couple to a nanowire on the surface of the waveguide. A single photon in the waveguide has enough energy to force the nanowire to transition from the superconducting phase to a resistive state. In the SOENs platform, this momentarily redirects the bias current along a lower resistance path, where the current can then be integrated within a superconducting loop, to allow further synaptic processing (figure \ref{fig:sup_synapse}).

SNSPD receivers dissipate zero static power. However, there is an electrical energy cost associated with detecting a photon. The nanowire has some inductance, $L_{spd}$ that stores energy from the current bias. During a detection event, this energy is released from $L_{spd}$ and dissipated in the resistor $r_{spd}$. The electrical energy necessary to detect each photon is then $\frac{1}{2}L_{spd}I_{spd}^2$. $L_{spd}$ can be as low as 100nH, resulting in an electrical energy consumption ($E_{spd}$) of around 10aJ/spike.

Since an SNSPD is capable of detecting single photons, it will operate near the quantum limit of optical communication. The minimum number of photons for a quantum-limited detector is often calculated in standard optical communications texts. We assume that the detection of a single photon will be treated as the registering of a spiking event. The probability of a light source producing a spike with a certain number of photons within a fixed time window is given by a Poisson distribution. We will also conservatively assume a detection efficiency $\eta_D$ of 70\%. The probability of measuring zero photons during a spiking event is then given by:

\begin{equation}
    P(0) = \sum_{k=0}^{\infty} \frac{N_{ph}^k e^{-N_{ph}}}{k!}(1-\eta_D)^{k} = e^{-N_{ph}\eta_D}
\label{eq: poisson}
\end{equation}

Where $N_{ph}$ is the average number of photons per spiking event. Neural systems are known for remarkable robustness to noise. Detecting only 99\% of spikes may be tolerable. From equation \ref{eq: poisson}, this would correspond to roughly 7 photons (0.9 aJ for $\lambda = 1.5$ um) needed to reach the receiver. Of course, the total number of photons produced by the source will need to be higher to account for energy losses in the link. The total optical energy per spike, $E_{opt}$, will be:

\begin{equation}
    E_{opt} = \frac{N_{ph} h \nu}{\eta}
\end{equation}

$h\nu$ is the energy of a single photon and $\eta$ is the total energy efficiency of the optical link. $\eta$ includes all optical losses and the inefficiency of the light source and transmitter. This efficiency factor will be highly dependent on the specifics of the platform, but for now we will leave it as a free variable. The total power consumed is the sum of $E_{opt}$ and $E_{spd}$. Accepting a 1\% error rate, these two contributions to the total energy will be roughly equal when $\eta = 10\%$. Such a high efficiency is likely near the limits of physical possibility. For more realistic values of $\eta$, $E_{opt}$ will dominate.

Importantly, superconducting electronics come with a cooling overhead (discussed in section \ref{sec:instantiation}). Conservatively, every watt of power produced by superconducting electronics will require about 1\,kW of refrigeration power to remove the excess heat. This means that the effective energy/spike for superconducting optical links will be no less than 1\,fJ.

Waveguide-integrated SNSPDs have demonstrated photon count rates exceeding 1 GHz \cite{vetter2016cavity}. However, large-scale integration of SNSPDs is still in its infancy. Slower detectors, such as MoSi SNSPDs with 20 MHz count rates, have demonstrated the best yields (?\%) to date. Previous statements that SOENs were limited to 20\,MHz were motivated by these pragmatic concerns about the current state of fabrication rather any known fundamental limitations of the detectors themselves.

\begin{figure}
    \centering
    \includegraphics[scale=1]{Receivers.pdf}
    \caption{a) A superconducting receiver featuring an SNSPD. Photons trigger a phase transition in the SNSPD (variable resistor and inductor) which forces current through $r_{spd}$ and the synaptic firing Jospheson junction, $J_{sf}$. While $I_{spd} + I_{sy}$ exceeds the critical current of $J_{sf}$, the JJ will produce fluxons. The number of fluxons produced before sufficient current returns to the SNSPD path is used as a synaptic weight and can be set by adjusting $I_{sy}$ - possibly with superconducting loop memory. The fluxon train is later processed by synaptic filtering circuits. b) A semiconducting receiver implementing the receiverless scheme. A photocurrent, $I_{pd}$ is produced by a photodiode in the presence of light and charges $C_{tot}$ up to a voltage capable of driving a buffer. Note that synaptic weighting for the semiconductor case is included in the filtering circuitry, shown in figure \ref{fig:my_label}.}
    \label{fig:sup_synapse}
\end{figure}

\subsubsection{Semiconductor Receivers}
\quad \quad Semiconductor receivers are an enabling technology behind the success of long-distance fiber-optic communication. However, there has been a new emphasis on designing receivers for short-distance intra-chip optical communication. Intra-chip optical receivers deviate significantly from their long-distance counterparts, as traditional transimpedance amplifiers likely consume too much power, despite impressive optical sensitivities. This has led to the proposal of ``receiverless" designs that omit amplifiers altogether \cite{miller2017attojoule}. Receiverless communication uses a photodetector to directly drive the input of CMOS gates. The photodetector produces a current in the presence of light which charges the CMOS gate capacitance up to the switching voltage. This is only possible with sufficiently small capacitance and sufficiently large optical signals. A circuit diagram of the scheme is shown in figure \ref{fig:sup_synapse}b in which a photodiode directly drives a CMOS buffer. A resistor is also placed in parallel to allow the receiver to reset.\footnote{In principle the resistor is unnecessary if an optical reset is used as described in \cite{debaes2003receiver}. The resistor would increase the minimum optical power necessary to register a spike and limit the bandwidth of the receiver.}

The necessary optical energy can be simply calculated. A charge $Q = C_{tot}V$ is necessary to charge $C_{tot}$ to a voltage ($V$) capable of driving the CMOS stage. $C_{tot}$ includes the photodiode capacitance, the CMOS gate capacitance, and any wiring capactiance. Each electron charging $C_{tot}$ must be generated by an incoming photon. This means at least $Q/e$ photons are required (if there is no internal gain). The total optical energy required, again with a total link efficiency $\eta$ is:

\begin{equation}
    E_{opt} = \frac{h \nu C_{tot} V}{\eta e}
\end{equation}

It may be possible to reduce $C_{tot}$ to the femtofarad level. For 1.5 $\mu$m photons and a required voltage swing of 0.8\,V, $E_{opt} \approx 0.7 $\,fJ (5000 photons) for unit efficiency. This is very similar to the superconducting case, once cooling is considered. This means that if two optical communications links were identical in all measures (source efficiency, optical losses, etc.) except one was cooled to 4K with SNSPDs and the other operated at room-temperature with photodiodes, then communicating a spike would cost nearly the same energy in each link. This point is further explored in figure \ref{fig:communication}, where we see that there is the opportunity for semiconductor receivers to consume less energy than their superconducting counterparts, but only if the capacitance can be pushed below 1\,fF. Several experimental demonstrations are encouraging for the realization of these ultra-low energies.

Just as with SNSPDs, semiconductor receivers will also require electrical power, even if it is minimized by the receiverless approach. In this case, there will be static power dissipation through the leakage current of the photodiode. Assuming a 1\,V bias, a leakage current on the order of 1\,nA, and an optical link efficiency of 1\%, this static dissipation would dominate power consumption for spiking rates below 100\,MHz. While very fast neuromorphic systems are certainly of interest, slower systems may be useful for brain-machine interfaces or for responding to stimuli in the physical world. The development of low capacitance, zero-bias photodiodes would be a major advantage towards making efficient, low frequency networks \cite{nozaki2018forward}. Superconducting receivers, with zero static power dissipation, have a clear advantage in this regard.

While the receiverless scheme is promising for achieving low energies per bit, it places significant burden on the transmitter side of the link. Neuromorphic applications magnify this burden, as neurons are expected to drive thousands of downstream connections in parallel. Additionally, the receiver capacitance must be charged quickly to maintain high spiking frequencies. Consequently, high fan-out neurons will require require transmitters with high optical power. This is shown in figure \ref{fig:communication}. Nano-LEDs struggle to achieve powers in excess of 1\,$\mu$W \cite{romeira2019physical}. The optical communication scheme will need to be adjusted to account for this limitation. One possibility is to move away from the receiverless design and add gain stages at every synapse. Depending on the overall transmitter and waveguide efficiency, this change may increase the energy/bit for short-distance communication (see ref \cite{miller2017attojoule} for discussion), but would permit higher fan-out for low power light sources. Avalanche photodiodes could also provide moderate internal gain to reduce power consumption, but high bias voltages may pose an issue. A second possibility is to use an optical repeatering scheme. The energy consumption in this model would likely still be low, but would increase the number of light sources required by each neuron. This could have deleterious effects on yield, depending on the reliability of light source fabrication. SNSPD receivers, in contrast, place nearly the theoretically minimum possible burden on transmitters, greatly improving the fan-out capabilities of a single light source, while still maintaining energy efficiency on par with receiverless operation.


\begin{figure}
    \centering
    \includegraphics[scale=1]{opticalv2.pdf}
    \caption{A) The minimum optical energy ($\eta = 1$) for an ideal photodiode to generate a 0.8\,V swing is plotted as function of the total receiver capacitance. The optical energy per bit for an SNSPD detector is shown as a dotted blue line. Shaded regions indicate which detector has lower optical energy per bit. B) The required peak optical power to drive 1000 downstream synapses as a function of maximum spiking frequency desired.}
    \label{fig:communication}
\end{figure}


\subsection{Optical Transmitters}
For either superconducting or semiconducting platforms, the transmitter is expected to dominate the power budget of optical links. Transmitters must be waveguide-integrated, capable of delivering sufficient optical power to drive many down-stream synapses, and capable of modulation at speeds greater than the desired maximum spiking frequency of the system. Room-temperature, CMOS-compatible light sources have been a holy-grail in the photonics industry for decades. The cryogenic environment of superconducting systems significantly improves the prospects for integrated light sources, but comes with its own challenges. Namely, driving light sources with superconducting electronics is a fundamental challenge, and will require new interfacing technologies - an obstacle that is completely absent from an all-semiconductor platform.

\subsubsection{Integrated Light Sources}
There are many review articles discussing the long-sought goal of monolithically integrated light sources on silicon substrates. Our goal is not to summarize all developments in the field, but to address important factors to consider in choosing a light emitter for this specific application.

The neurons we have envisioned require only incoherent light pulses. Nanolasers are not necessary for this application if nanoLEDs can be constructed with suitable power (see previous section). \textcolor{Violet}{Is that fair? We don't need nanolasers since we don't coherence, but they could be more powerful than LEDs?} NanoLEDs are also attractive for their simplicity of fabrication, lack of threshold current, and improving efficiency with shrinking scale \cite{romeira2019physical}. However, integrating millions of nanoLEDs on 300\,mm wafers (see scaling analysis in section \ref{sec:instantiation}) is still only an aspiration. The difficulty is a fundamental one. Silicon's excellent electrical properties have resulted in a fabrication ecosystem nearly entirely dedicated to the processing of silicon substrates. Unfortunately, the indirect bandgap of silicon renders it one of the most unpromising semiconductors for light emission. This leaves only two options: (1) Develop processes to economically integrate direct bandgap materials on silicon substrates at a massive scale or (2) force silicon to emit light through either material and/or environmental modifications.

III-V semiconductors are the most commonly used materials for optical communication. However, while extensively investigated, integrating III-V materials with silicon still poses significant challenges. A lattice mismatch between most III-V materials and silicon has so far prevented direct epitaxial growth from being a dominant solution.\footnote{Recent developments in this area are encouraging, however. \cite{li2017epitaxial}} Another route to III-V heterogeneous integration involves physically attaching pre-grown III-V crystals onto silicon wafers. A variety of schemes have been proposed, including flip-chip bonding, die-to-wafer bonding, wafer-to-wafer bonding, and transfer printing \cite{zhang2019iii}. These processes are still unproven at massive scale, and it remains to be seen if they will be an economical solution.

In contrast, proponents of all-silicon light sources argue that bonding technology will remain expensive and challenging for large-scale integrated photonic circuits. The vision is that the comparatively simple fabrication process for silicon LEDs will enable massive integration density, reminiscent of the success of the homogeneous CMOS process. Main avenues of research include erbium-doped silicon, silicon nanocrystals, strained SiGe, and loop dislocation emitters, among others \cite{pavesi2008silicon}. Additionally, silicon light sources are naturally compatible with silicon waveguides, which potentially reduces coupling losses. However, the performance of silicon light sources has never proven to be competitive with III-V light sources, leading to an unfortunate trade-off between the scalability of silicon and the performance of III-V materials.

However, for superconducting optoelectronic systems, the cryogenic operation brings significant benefits to light-sources, both silicon and III-V. The efficiency of a light emitter is determined by relative rates of radiative and non-radiative recombination. Upon lowering the temperature from 300K to 4K, radiative recombination time was demonstrated to decrease by over four orders of magnitude in GaAs/AlGaAs quantum wells, resulting in a corresponding increase in efficiency measured by photoluminescence \cite{gurioli1991temperature}. Even more applicable, the supression of non-radiative recombination paths resulted in 1\% efficient, waveguide-integrated InP LEDs at 10K - a two orders of magnitude improvement over room-temperature \cite{dolores2017waveguide}. These light sources are already suitable for extremely large neural systems (section \ref{sec:instantiation}) if they can be manufactured at scale. Additionally, spectral broadening from phonon interactions is significantly depressed at low-temperature, making narrow linewidth emitters more easily achieved. \textcolor{Violet}{Is that alright?} This means that the Purcell Effect - the increased emission of light emitters when coupled to a resonant cavity - is often easier to demonstrate at low temperatures. For silicon light sources, there may be even more gains in moving to a low-temperature environment. A variety of point defects demonstrate photoluminesence in silicon only at low temperatures, some of which can be patterned and formed easily with ion implantation. In particular, the W-center, an interstitial defect formed through silicon self-implantation has generated significant interest. W-centers have been demonstrated in 300\,mm foundry processes \cite{buckley2020optimization} and have already been integrated with waveguides and SNSPD receivers \cite{buckley2020optimization}. Efficiency is still a major question, however, as the maximum external efficiency reported for a W-center LED is only $10^{-6}$ \cite{bao2007point}, although the internal efficiency may have been significantly higher.

\subsubsection{Driving Circuitry}
A neuron must be capable of driving a semiconductor light source. The transmitter circuitry is thereby required to produce voltages on the scale of the bandgap of the optical source - usually on the order of 1\,V. CMOS circuitry, itself a semiconducting technology, naturally operates on this voltage scale rendering the driving circuitry essentially a non-issue. Standard MOSFET LED driving circuits should be suitable for neuromorphic applications. Superconductors, however, operate in an entirely different regime, with signals usually on the order of the superconducting energy gap, which is typically about 1\,mV. The optimal method for interfacing superconducting electronics with semiconductor devices is still an area of active research. Recent progress has been made, however, with devices utilizing the massive change in impedance during a superconducting-insulator transition. In ref \cite{mccaughan2019superconducting}, a resistive element is heated using 50\,mV pulses to thermally trigger a transition in a superconducting meander. The meander transitioned to a state with resistance in excess of 10\,M$\Omega$ and was used to drive a cryogenic light source. While these results are promising, the light source was only pulsed at only 10 kHz (due to poor source efficiency) and was fabricated on a separate chip. More work is needed to improve the speed, to monolithically integrate this driving circuitry with LEDs, and to ultimately demonstrate fully functioning optical links using SNSPDs.

\section{\label{sec:soma}Electronic Neuronal Computation}
Electronic circuitry capable of faithfully emulating neuronal dynamics will also be necessary. While the programability of digital neuromorphic chips will make them a vital tool in the near future, analog circuitry will likely ultimately be more efficient at simulating neuronal behavior. Analog CMOS neurons exhibiting complex behavior have been developed over the past few decades. Many of these designs and concepts should be transferable to optoelectronic semiconductor platforms. Additionally, superconducting neural circuits have also been developed, and appear capable of implementing just as sophisticated behavior.

\subsection{CMOS}
The maturity of CMOS processing has allowed great strides in neuromorphic computing. Digital neuromorphic chips designed for edge applications are nearing commercialization \cite{davies2018loihi, merolla2014million}. While optical communication would likely also be advantageous in digital approaches, we focus on analog CMOS neurons for their perceived efficiency advantages \cite{rajendran2012specifications, mead1990neuromorphic}. Analog CMOS neuromorphic systems have been the subject of intense research since the work of Carver Mead in the late 1980s. Mead recognized that many of the behaviors observed in biological cells lent themselves to simple and efficient emulation in analog circuitry \cite{mead1990neuromorphic}. At the most basic level, a neuron must perform three mathematical functions: summation of synaptic functions, temporal filtering, and threshold detection. Summation can be achieved by exploiting Kirchoff's law for current summation. Filtering can be implemented with elementary resistor-capacitor circuits. Thresholding is natural function of transistors. Building upon this basic mapping, a great variety of analog neurons have been demonstrated to emulate a litany of biologically-inspired neuronal models \cite{indiveri2011neuromorphic}. It has even been argued that analog neuromorphic circuits are already mature enough to simulate the brain with current technology \cite{hasler2017special}.

However, it must be noted that increasing circuit complexity is usually accompanied by increasing power consumption. It was found in the previous section that optical communication requires a minimum of about 1\,fJ of energy to deliver a spiking signal to each synapse. For realistic optical link efficiencies, this value will be at least one or two orders of magnitude larger. Synaptic filtering circuits would therefore ideally operate with an energy budget of 10 - 100\,fJ of energy to process a single spike. For the simplest implementations, synapses may be nothing more than resistive memory element (section \ref{sec:memory}) and a wire. However, more sophisticated synaptic circuits are likely necessary for improved problem-solving ability and greater biological realism \cite{chicca2020recipe}. Somatic computation could comfortably consume power larger than that of synaptic processing by a factor of the average fan-out (perhaps 1000). The energy scale of synaptic and neural circuits is set by the value of capacitance used for filtering and the supply voltage ($CV^{2}$). By reducing both the capacitance and voltage, a neuron capable of 25 kHz spike rates was demonstrated to consume only 4\,fJ/spike \cite{sourikopoulos20174}. Many other analog neurons, with energies ranging from fJ to pJ per spike, fall comfortably below the power consumption of optical communication \cite{indiveri2019importance}. Neurons with maximum spike rates exceeding 100 MHz have also been demonstrated \cite{schemmel2020accelerated}. Optical communication should face few issues achieving such speeds, if it can be efficiently integrated with these circuits.

Some CMOS neuromorphic circuits have prized bio-realism --- targeting biologically realistic timescales and fan-in. Achieving long time constants commensurate with biology (upwards of 500\,ms) has been achieved using sub-threshold circuits with very low currents \cite{indiveri2011neuromorphic}.  This reduces the need for large capacitors that can be difficult to integrate on chip. In terms of fan-in, most implementations have so far fallen just short of biological levels ($10^3$--$10^4$ synapses per neuron). Fan-in of 256 has been demonstrated for both sub-threshold \cite{qiao2015reconfigurable} and accelerated time circuits \cite{schemmel2020accelerated}. Analyses of specific implementations have suggested that fan-in on the scale of biology is at least theoretically possible \cite{dowrick2018fan, akima2014majority}.  

For a concrete example, a circuit diagram for a memristor implementation of the popular Differential Pair Integrator Synapse (DPI) is shown in figure \ref{fig:filtering}b \cite{dalgaty2019hybrid}. The DPI produces a decaying exponential waveform in response to an input voltage pulse - potentially from an optical receiver. This leaky integrator behavior is characterized by a time constant set by the value of the filtering capacitance and the rate of leakage off of the capacitor \cite{chicca2014neuromorphic}. This time constant could potentially even be programmed using memristors - which is an advantage over superconducting circuits that have been proposed to date.

\begin{table*}[h]
  \begin{center}
    \label{tab:mathtable}
    \begin{tabular}{l|c|r} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \textbf{Function} & \textbf{Semiconductors} & \textbf{Superconductors}\\
      \hline
      \textit{Summation} & Kirchoff's Law & Mutual Inductors (Lenz's Law)\\
      \textit{Low-Pass Filter} & RC Circuit & LR Circuit\\
      \textit{Thresholding} & Transistors & Josepheson Junctions\\
    \end{tabular}
    \caption{Summary of implementations of basic neuronal functions in each platform.}
  \end{center}
\end{table*}

\subsection{Superconducting Electronics}
Superconducting neurons have been studied nearly as long as CMOS implementations, with a mapping between neuronal functions and superconducting electronics also identified in the early 1990s \cite{hago1991, hiak1991}. In this case, Lenz's Law, governing the addition of magnetic flux through mutual inductors to superconducting loops provides the necessary synaptic summation function. Filtering is achieved through resistor-inductor blocks.\footnote{Or RC circuits in some cases \cite{crotty2010josephson}.} Finally, Josephson Junctions (JJs) provide the requisite nonlinear thresholding element.\footnote{Spiking behavior from superconducting nanowires have also been proposed as an alternative nonlinear element to JJs \cite{toomey2019design}.} A summary of these mappings is given in table \ref{tab:mathtable}. 

Also like their CMOS counterparts, many superconducting circuits have now been designed to implement sophisticated neuronal dynamics. Superconducting neuromorphic circuits have been designed to implement a variety of bio-inspired neuron models \cite{crotty2010josephson, toomey2019design, schneider2018tutorial}, sophisticated dendritic processing \cite{shainline2019fluxonic}, and have performed image classification in simulation \cite{schneider2017energy}. Promising concepts for superconducting plasticity circuits and memory are addressed in the next section. The natural spiking behavior of JJs may even require a lower device count than analogous CMOS circuits for various leaky-integrate-and-fire models \cite{crotty2010josephson}. In short, it does not appear that superconducting circuits are any less capable of complex neuronal computation than CMOS platforms.

Superconducting electronics is traditionally associated with gains in energy efficiency and speed. Indeed, superconducting elements dissipate zero static power and spike energies are frequently reported in the sub-fJ range, even including refrigeration. This makes it a near certainty that optical communication will dominate power consumption for superconducting optoelectronic systems. In terms of speed, fully electronic superconducting neurons may be capable of spike rates up to 100 GHz \cite{schneider2017energy, schneider2018tutorial}. However, this is orders of magnitude faster than any SNSPD can respond. This marks a notable difference between the superconducting and semiconducting architectures. While optical communication could be integrated with CMOS neurons with no degradation in speed, optoelectronic superconducting systems will likely be significantly slower than their fully electronic counterparts.  We believe that this is the cost of highly connected systems.

Somewhat surprisingly, the ability of superconducting electronics to go slow might be just as compelling as their ability to go fast. \textcolor{blue}{Too corny?} While great effort was needed to implement long, biologically realistic time constants in CMOS neurons, superconducting loops can create essentially arbitrarily long time constants by tweaking the $L/R$ ratio in synaptic and neuronal loops (figure \ref{fig:sup_synapse}). The exact consequences of this capability are unknown, but intriguing, as it has been suggested that the human brain itself is limited by the length of time constants available in its hardware \cite{indiveri2019importance}. The ability to generate dynamics across many orders of magnitude in time also dovetails nicely with suggestions that critical behavior is important for cognition \cite{cocchi2017criticality}.

Fan-in has traditionally been considered a liability of superconducting electronics. If this were the case, it would clearly be an impediment to mature superconducting neuromorphic systems. For superconducting neurons designed to use single fluxons as action-potentials, fan-in is still likely to be limited to around 100\cite{schneider2020fan}. However, if signals are allowed to contain multitudes of fluxons, the fan-in problem is much less serious, and can likely scale to biological levels through the use of mutual inductors. Using larger signals comes with larger power consumption, but for optoelectronic systems, light production will almost certainly still dominate. This makes the decision to move away from single-flux computing a natural one for superconducting optoelectronic systems, as it solves the fan-in problem with no noticeable increase in power consumption.

It is important to recognize that while most diagrams of superconducting circuits (including those here) show many separate biases delivering current to various elements, the ability to construct circuits that can be biased in series will be critical to the scalability of this hardware. A separate bias for every synapse would be absolutely untenable in large-scale systems. This mimics the evolution that occurred in superconducting digital electronics, in which the field turned away from the parallel biasing of RSFQ, whose current sources dominated the energy budget, and towards the serially-biased RQL architecture \cite{tolpygo2016superconductor}. We believe that the neurons designed for the SOENs platform are amenable to serial biasing, but this important point demands further analysis.

A superconducting synaptic filtering circuit is shown in figure \ref{fig:filtering}a. Synaptic weighting was implemented in the receiver circuit (figure \ref{fig:sup_synapse}a), so this circuit block is only responsible for converting a train of fluxons into a decaying exponential reminiscent of biology and CMOS synapses. A resistor, $r_{si}$ converts a superconducting persistent current loop into a leaky-integrator very similar to the DPI synapse. The time constant is set by $L_{si}/r_{si}$ and the synaptic current can be added to a neuronal circuit through mutual inductors. Unlike the DPI synapse, this circuit does not have a programmable time constant, but does hold the potential to implement a wide range of different time constants by fabricating different values of $L_{si}$ and $r_{si}$.



\begin{figure*}
    \centering
    \includegraphics[scale=.71]{Synaptic Filters.pdf}
    \caption{a) The superconducting synaptic filter receives a train of fluxons. The number of fluxons is proportional to the synaptic weight. $J_{si}$ will convert the fluxon train into a circulating current (whose magnitude is proportional to the number of received fluxons) in the synaptic integration loop, SI. The prescence of $r_{si}$ makes the SI loop a leaky integrator, with time constant $L_{si}/r_{si}$. $I_{si}$ can then be coupled to neuronal integration loop (NI) with mutual inductors. b) A schematic of a differential pair integrator (DPI) synapse featuring memristors \cite{dalgaty2019hybrid}. An input pulse from the optical receiver enables a current proportional to $r_w$ to flow from $C_{si}$ to ground. The rate that $C_{si}$ recharges is determined by $T_3$ and $r_{si}$. This allows synaptic time constants to be programmed. $C_{si}$ and $r_{si}$ form a leaky integrator analogous to superconducting case, and the output current $I_{si}$ can be summed via Kirchoff's Law at a summing node in the somatic circuitry. Plasticity circuits would utilize the various transistors to program values in the memristors.}
    \label{fig:filtering}
\end{figure*}

\section{\label{sec:memory}Synaptic Memory}
It has been apparent to the neuromorphic community for some time that large-scale neural systems will require innovative approaches to synaptic memory. Many modern CMOS neuromorphic systems use bias generators and digital memory to store and control synaptic parameters \cite{liu2014event} Bias generator blocks store parameters digitally and route the appropriate voltages to synaptic and neuronal circuits across the chip. While bias generator circuits are valuable for smaller-scale networks, static power consumption, analog-digital conversion, interconnect charging energy, and inconvenient non-local plasticity updates will make them untenable for larger networks \cite{dalgaty2019hybrid}. These issues have led to a widespread exploration of emerging memory technologies for neuromorphic computing. Extensive investigation is still needed to determine which technology will prove best-suited for large-scale neural systems. However, there are numerous desired characteristics that guide the development of synaptic memories. Among these metrics are weight precision, volatility, area, endurance (the effective number of cycles in a device's lifetime), device-to-device and cycle-to-cycle variability, material compatibility, update symmetry and linearity, write speed, and switching energy. We attempt to provide desired benchmarks for a few of these metrics in the specific case of optoelectronic networks.
\subsection{Memory Benchmarks}
\subsubsection{Endurance}
The large-scale cognitive systems discussed here could be used in at least two different modes. The first is to serially process many separate user-defined tasks over their lifetimes, similar to how modern supercomputers are used. The second is a lifelong learning approach, where the system's weights are allowed to naturally evolve over the course of its \textcolor{blue}{operation due to plasticity mechanisms activated by internal network activity}, with no interference from users after initialization. In either of these modes, it will be vital that the synaptic weights can be changed throughout the lifetime of the system. This stands in contrast to many edge systems, where inference is a primary (or even sole) mode of operation. Additionally, edge systems will be much less costly to replace, making shorter lifetimes acceptable. Massive neural systems, carrying with them correspondingly sizeable investments \textcolor{blue}{in money and time}, will be desired to have lifetimes on the scale of decades ($10^9$ seconds)\textcolor{blue}{, if not longer}.

The number of times a synapse is updated in its lifetime is a function of neuron spiking frequency ($f$) and the number of synapses that are typically updated after each \textcolor{blue}{spike from the postsynaptic neuron}. In neuroscience literature, \textcolor{blue}{evidence has been presented} that the number of active presynaptic inputs required to trigger a postsynaptic spike roughly goes as $\sqrt{N}$, where $N$ is the fan-in of the neuron\textemdash perhaps 1,000 for brain-like systems \cite{vrso1996,vora2005}. \textcolor{blue}{Numerous parameters may be introduced to characterize the synaptic update frequency \cite{fuab2007}. For simplicity, we assume here that all synapses that contributed to the spiking of the post-synaptic neuron} are updated with each spike. \textcolor{blue}{We then estimate} the number of weight updates ($N_{updates}$) in the system's lifetime ($L$) will be:

\begin{equation}
    N_{updates} = \frac{Lf}{\sqrt{N}}
\end{equation}

For a decades-long lifetime, and a mean spiking frequency of 10 kHz, the total number of weight updates will be $10^{11}$. This is a challenging demand \textcolor{blue}{that far exceeds the endurance of most memristive technologies that rely on the repeated motion of atoms within a host material for electronic property adaptation. Yet even this extreme reliability} has been demonstrated in state-of-the-art memristive and phase change memory (PCM) devices \cite{zhao2020reliability}. \textcolor{blue}{Approaches to memory update that do not require atomic reconfiguration of a material may be better equipped to endure the demands of lifelong learning. In the semiconductor case, this might be accomplished through modification to the voltage on a MOSFET gate, possibly with injection of charge on a floating gate. In the superconductor case, similar functionality can be achieved through modification to the current bias to a JJ, achieved through adaptation of the amount of flux stored in a superconducting loop.}

\subsubsection{Update Energy}
\textcolor{blue}{Ideally, one would like} the power dedicated to weight updates to be negligible compared to power used for optical communication. Once again invoking the assumption that $\sqrt{N}$ synapses are updated with each postsynaptic spike, we arrive at following relation between the energy to produce a single spike ($E_{spike}$) and that to update a single weight ($E_{update}$):

\begin{equation}
    E_{update} < \sqrt{N}E_{spike}
\end{equation}

Using the analysis in section 2, roughly 10 fJ of energy needs to be delivered to the receiver in either platform. Assuming a transmitter efficiency of 1\%, this would mean $E_{spike}$ is roughly 1 pJ. Therefore, for a fan-in of 1,000 synapses, $E_{update}$ would ideally be no more than 30 pJ. Note that this value also includes any energy spent in peripheral circuitry to program the synapse. This suggests that in optoelectronic systems, there is little to be gained by designing update circuits that consume significantly less energy than a few picojoules. This figure appears to be have already been met by several emerging memory technologies \cite{zahoor2020resistive}.

\textcolor{blue}{Based on these considerations, we arrive at several target values for key synaptic memory metrics. These metrics and their target values are summarized in Table \ref{tab:memory_metrics}. In the remainder of this section, we consider the prospects of specific technologies for achieving these performance goals.}

\begin{table}[h!]
  \begin{center}
    \label{tab:memory_metrics}
    \begin{tabular}{l|c|r} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \textbf{Metric} & \textbf{Goal} \\
      \hline
      \textit{Endurance} & $>10^{11}$ updates \\
      \textit{Update Energy} & $<$ 30 pJ\\
      \textit{Update Speed} & $<100$ ns \\
      \textit{Area} & $<100-1000$ $\mu m ^2$ \\
      \textit{Weight Precision} & 4-10 bits \cite{pfeil20124}
      
    \end{tabular}
    \caption{List of desired performance metrics for synaptic memory in a system with average fan-out of 1000, maximum spike rate of 10 MHz, average spike rate of 10 kHz, and spike energy of 10 fJ.}
  \end{center}
\end{table}

\subsubsection{Programming Signals}
One criterion that is difficult to quantitatively benchmark is the complexity of programming circuitry for synaptic memory. Many memory technologies (floating gate, RRAM, and PCM for example) require signals during the programming phase that are significantly different (either in strength or duration) from the signals produced in the normal operation of the network. This often requires extra circuitry that reads the local network activity and decides when to take the memory device offline to apply a WRITE operation with these tailored programming signals. This peripheral circuitry can sometimes be significantly different from that used for the rest of the hardware. We are not claiming that such a scheme is untenable for any inherent reason, but only that it is important to include the complexity of programming circuitry when comparing memory technologies. Superconducting loop memory (discussed in section ??) is particularly intriguing from this standpoint, as the plasticity circuits operate with  nearly identical signals and circuit blocks as those found in the rest of the network.

\subsection{Proposed Technologies}
\subsubsection{Room-temperature Technologies}

Many technologies have been proposed to implement synaptic weighting for room-temperature neuromorphic hardware, each with strengths and weaknesses \cite{upadhyay2019emerging}. The quest to find a suitable device for local synaptic memory dates back to the origins of the field, when Carver Mead and colleagues were investigating floating gate transistors \cite{diorio1998floating}. Since then, floating gate synapses have been used to implement STDP \cite{ramakrishnan2011floating} and are attractive as a mature alternative to emerging devices. However, there are concerns about high programming voltages, speed, and endurance that have made researchers turn to other technologies \cite{zahoor2020resistive}. \textcolor{blue}{Because modification of the charge on a floating gate requires significantly different voltage levels and signal times than the spiking signals transmitted from neurons to synapses, this otherwise ideal memory mechanism has not led to large-scale learning systems. Memristive devices \cite{stsn2008,yast2012,ab2018}, commonly used in resistive random-access memory (RRAM)} has emerged as a \textcolor{blue}{popular} alternative, with recent demonstrations including monolithic integration with CMOS \cite{yin2019monolithically} and unsupervised pattern recognition with a simple network of synapses \cite{ielmini2018brain}. However, questions remain about high variability (both cycle-to-cycle and device-to-device) \cite{dalgaty2019hybrid}, endurance, linearity, and the number of internal states \cite{zahoor2020resistive}. Phase Change Memory (PCM) is another intriguing option, with its own demonstration of STDP \cite{ambrogio2016unsupervised}, and is more mature than RRAM. Thermal management and energy consumption have been raised as issues \cite{upadhyay2019emerging, zahoor2020resistive}. Ferroelectric transistors present another interesting alternative, but likely require new material development. Spin-torque memory, 2D materials, and organic electronics have also been proposed as solutions. Interested readers should consult one of the many review articles on this topic \cite{kim2018recent, upadhyay2019emerging, zhang2020brain}. In short, the field is burgeoning with new devices for synaptic memory, but to-date none has been dominant enough to monopolize the \textcolor{blue}{research field.} Lastly, we include a table, adapted from \cite{zahoor2020resistive} that compares a few promising technologies and compares them to the goals we laid out for large-scale optoelectronic networks. While there is still significant progress needed to make these memory, the benchmarks for large-scale optoelectronic systems seem to be within the realm of possibility for both RRAM and PCM.

\begin{table}[h!]
  \begin{center}
    \label{tab:memory_comparison}
    \begin{tabular}{l|c|c|r} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \textbf{Metric} & \textbf{Floating Gate} & \textbf{RRAM} &\textbf{PCM} \\
      \hline
      \textit{Endurance} & $10^{5}$ & $10^{6}-10^{12}$* & $10^{9}$  \\
      \textit{Update Energy} & \textbf{$\textbf{10}$ fJ} & $\textbf{.1}$ \textbf{pJ} & $10$ pJ\\
      \textit{Update Speed} & $10$ $\mu$s  & $\textbf{10}$ \textbf{ns} & $\textbf{50}$ \textbf{ns}\\
    \end{tabular}
    \caption{Comparison of the current state of three leading technologies for synaptic memory. Metrics that have already met the conditions laid out in table \ref{tab:memory_metrics} are in bold. *The high endurance RRAM measurement was achieved in a binary mode.}
  \end{center}
\end{table}

\subsubsection{Superconducting Technologies}
Many of the aforementioned technologies may also be operational at cryogenic temperatures for use in superconducting optoelectronic systems, but little work has been done in this regard. Instead, two other types of memory, only accessible at low temperatures, have received the most attention: Magnetic Josephson Junctions (MJJs) and superconducting loop memories. MJJs are extremely low-energy \textcolor{red}{(quantify energy for update and during synaptic events)}, non-volatile, two-terminal devices that are functionally analogous to room-temperature memristive devices. Superconducting loop memories, on the other hand, are close analogues of floating gate transistors (with circulating current playing the role of trapped charge), but operate on different principles than any of the other memories mentioned.

\subsubsection{Magnetic Josepson junctions}
\textcolor{blue}{A Josephson junction (JJ) is formed from two superconducting electrodes separated by a tunneling barrier.} If a ferromagnetic material is placed within the \textcolor{blue}{tunneling barrier}, either in the form of nanoclusters or continuous films, the critical current of the junction can be controlled by changing the alignment of magnetic moments within the insulator. The magnetic order of the ferromagnetic materials can be programmed with current pulses in the presence of an external magnetic field, providing an analog, nonvolatile memory. For cryogenic \textcolor{blue}{applications}, volatility takes on a slightly new meaning. For a memory to be nonvolatile in this context, it must preserve its state upon both the removal of electrical power and upon warming up to room temperature. \textcolor{blue}{MJJs are nonvolatile with respect to removal of electrical power and cant retain their state upon heating to around 200\,K. Further stability up to 300\,K remains the subject of research.} 

MJJs provide remarkable synaptic performance \textcolor{blue}{with respect to the metrics given in Table \ref{tab:memory_metrics}}. The synaptic update programming is on the order of femtojoules (\textcolor{blue}{after multiplying by $10^3$ to account for cryogenic cooling}), commensurate with firing rates exceeding 100\,GHz, and can be fabricated at the scale of tens of nanometers. \textcolor{blue}{A tradeoff occurs between the size of the MJJ and the number of analog levels of magnetization, as smaller junctions contain fewer nanoscale magnetic clusters that can be reoriented. Regardless, even for relatively large junctions with many analog levels, the size of  MJJs is inconsequential compared to the size of other components of superconducting synapses, such as mutual inductors used in coupling between synapses and the neuron cell body.} \textcolor{red}{(will this comment make sense? is it necessary to give a rough overview of the synapses and neurons under consideration at the beginning of the document?)} All of these metrics \textcolor{blue}{surpass the requirements} for optoelectronic networks, and can be exploited in all-electronic superconducting networks as well. More work is needed to analyze the scaling potential of MJJs \textcolor{blue}{with respect to yield}. The magnetic fields used during programming can be produced with magnetic control lines, but spin-torque mechanisms may provide a more scalable solution. Peripheral circuitry involved in applying programming pulses and \textcolor{blue}{endurance with respect to repeated realignment of the magnetic moments merit further investigation} \cite{schneider2018ultralow}. \textcolor{blue}{While the energy, speed, area, and weight precision, meet the requirements specified in Table \ref{tab:memory_metrics}, the ability to modify the state of the MJJ using the same signals as are sent from neurons to synapses remains a technical challenge and potential barrier to straightforward implementation of large-scale, unsupervised learning. These challenges are analogous to the challenges faced by memristors and phase-change memory elements, wherein current or voltage pulses are required in addition to those being produced by neurons for signaling to synapses.}

\subsubsection{Loop Memory}
Superconducting loop memories \textcolor{blue}{have been in use for decades by the superconducting electronics community \cite{vatu1998,ka1999}, but are not ideal for large memory arrays commonly utilized as RAM in digital computing due to the large size as compared to semiconductor alternatives. In the case of optoelectronic spiking neural systems considered here, the objective is not to produce large RAM arrays, and size as well as addressing challenges do not emerge as significant impediments. Therefore, straightforward extensions of binary loop memories} are the synaptic memory technology \textcolor{blue}{that appears most promising} for the SOENs platform \cite{sh2018,shainline2019superconducting}. In these memory cells, circulating current persists indefinitely in a loop of superconducting wire. The current in the loop can be controlled by adding/removing fluxons (the quantized units of magnetic flux) with standard JJ circuitry. This memory loop is then inductively coupled to a wire supplying a bias current to a Josephson Junction at the synapse. Each time the SNSPD detects a photon, the biased junction will add an integer number of fluxons to another integrating superconductive loop (analogous to the membrane capacitance of a neuron). The number of fluxons added to the integration loop is a function of the bias supplied to the JJ, which is determined by the magnitude of current circulating in the memory loop. \textcolor{blue}{The number of analog memory levels in the loop is determined by the inductance of the loop, which is easily set with the length of a wire. High-kinetic-inductance materials \cite{} enable memory storage loops with over a thousand levels (10 bits) to be fabricated in an area of 5\,\textmu m $\times$ 5\,\textmu m (check).}

This approach has several advantages. The memory is nearly analog and updates are nearly linear. \textcolor{blue}{Memory is updated by the switching of a JJ, which involves only a change of the phase of the superconducting wave function. This does not involve the motion of ions within a material, so there appears to be no limit to the number of times the memory can be updated}. It is also attractive from a fabrication perspective as loop memory requires no additional materials. Further, the simplicity of the memory lends itself favorably to 3D integration. Plasticity circuits based on loop memories will also operate at the energy scale of single photons and few flux quanta ($\sim 10^{-19}$\,J), commensurate with the rest of the circuitry in the network. This allows weight updates to be performed with the spikes the network produces in standard operation, cutting down significantly on peripheral circuitry. There is no need to engineer differently shaped pulses for READ and WRITE operations, and the synapse does not need to be taken offline during programming. Simulations have demonstrated STDP learning with circuits containing only four additional Josephson Junctions \cite{shainline2019superconducting}. 

Still, \textcolor{blue}{two aspects of} loop memory are concerning. First, \textcolor{blue}{memory represented by current circulating in a loop persists as long as superconductivity is maintained, even without applying any power source. However, as soon as superconductivity is broken (by raising the temperature of the system, for example), the memory is lost. The temperature of the system may need to be raised to perform maintenance or inadvertently if the system power supply fails.} If this memory is to be preserved during a warmup of the network, it must be first transferred to some other memory storage system. \textcolor{blue}{An exciting prospect is for memories gained through unsupervised plasticity during normal network activity to be transferred to a less volatile storage mechanism such as and MJJ, perhaps during a type of sleep phase. But a mechanism for this memory transfer remains to be discovered. The second weakness of loop memory is the size.} The employed superconducting loops, \textcolor{blue}{as well as the transformers that couple them}, will be large compared to all of the other solutions discussed. \textit{Size estimate depends on if we're using SNSPDs or fluxons.} However, \textcolor{blue}{the consequences of these large-area components must be considered in the context of the entire system, which we discuss in Sec.\,\ref{sec:}.} 

%this is not incompatible with the size of superconducting receivers which are already on the order of $1000 \mu m^2$ and the area concerns are mitigated by the excellent potential of loop neurons for 3D integration.

\section{\label{sec:instantiation}Physical Instantiation}
\textcolor{ForestGreen}{Here we attempt to anticipate requirements for the physical realization of optoelectronic spiking neural systems comprising 10 billion to 100 billion neurons with 100 trillion synapses. We begin this analysis with purely mathematical considerations from graph theory, which inform us as to how many synaptic connections are required to maintain a given average path length across the network. This consideration appears central to efficient communication and information integration in spiking neural systems, is likely the primary reason the brain prioritizes extensive communication infrastructure, and provides a hardware-agnostic foundation on which we base the subsequent spatial analysis.}

\textcolor{ForestGreen}{From graph theory metrics, we proceed to general spatial considerations. Moving from abstract mathematics to a physical instantiation, we make several specifying assumptions: 1) optoelectronic circuits will be fabricated on the two-dimensional surfaces of wafers using the sequential, layered processing techniques that have enabled VLSI circuits; 2) the wafers have a diameter of 300\,mm; 3) multi-wafer systems are constructed by stacking multiple wafers in columns and assembling these columns into three-dimensional architectures; 4) free-space optical communication can be used to signal from a wafer to the two wafers directly adjacent in the vertical direction; 5) inter-wafer couplers are available between each wafer and the four wafers that reside in the same plane in the cardinal directions; and 6) all other communication across the network occurs over optical fibers that fill space between the wafers. With such a construction in mind, the analogy between the grey and white matter in the brain is apparent: the grey matter comprising the computational circuits consists of the hardware integrated on the wafers, while the white matter comprising the communication infrastructure consists of tracts of fiber optic cables spanning the network. Illustrations to clarify the concept are shown below.}

\textcolor{ForestGreen}{With a specific hardware concept specified, we turn attention to considerations pertinent to the fabrication of the wafer-integrated circuits. The photonic and electronic circuits that must be realized have been described above in Secs. \ref{sec:soma}, \ref{sec:communication}, and \ref{sec:memory}. Here the primary considerations relate to the integration of photonic with electronic components and the benefits of dense multi-planar integration. We consider specific processing challenges that appear inevitable for multi-planar realization of semiconductor circuits, superconductor circuits, or photonic components. Challenges are present in all cases, but the challenges differ based on the technology.}

\textcolor{ForestGreen}{With a specific construction concept in mind, and having analyzed optoelectronic integration at the wafer scale, we proceed to estimate the connectivity, volume, power consumption, and cooling requirements of a brain-scale spiking neural system realized in this form. We return to metrics from graph theory to assess whether the necessary connectivity can be retained across the network hierarchy. We estimate the total spatial extent of a system, accounting for grey and white matter, as well as the number of 300-mm wafers necessary for the construction, and we determine that it is likely to be possible to cool such systems using water for semiconductors or helium for superconductors.}

\subsection{Considerations from graph theory}

\begin{figure}
    \centering
    \includegraphics[width=8.6cm]{random_graph__connections_vs_num_nodes.pdf}
    \caption{Average node degree required to maintain an average network path length as a function of the number of nodes in the network.}
    \label{fig:degree_vs_num_nodes}
\end{figure}

path length \cite{frfr2004}

\subsection{Generic spatial constraints}

\begin{figure}
    \centering
    \includegraphics[width=8.6cm]{area_scaling.pdf}
    \caption{Area scaling.}
    \label{fig:area_scaling}
\end{figure}


\subsection{Wafer fabrication and processing}

\begin{figure}
    \centering
    \includegraphics[width=8.6cm]{fabrication_stack.pdf}
    \caption{Fabrication stack.}
    \label{fig:fabrication_stack}
\end{figure}

the nature of superconductivity is that the quality of devices is not as affected by material crystalinity, so lower temp processing is adequate, thus enabling 3D integration

\subsection{Constructing large systems}

\textcolor{Red}{I'd really like to have a nice figure here so everyone can see what we're talking about clear as day.}

\subsection{Power consumption and cooling}

\subsubsection{Cooling Systems}
Cooling systems will be a key component to either platform. The ability to remove heat is necessary to maintain normal circuit operation. This is particularly dramatic for superconducting electronics, in which the devices will lose superconductivity if the temperature rises above the critical temperature ($T_c$). Pending a revolutionary development in high-$T_c$ materials, superconducting neuromorphic systems will rely on Niobium ($T_c = 9.3$K) or a material with a similarly low $T_c$. Liquid helium (4.2K) is the cryogen of choice for such temperatures. Cooling systems will add significantly to the power consumption of superconducting electronics. The power efficiency of a refrigeration system is measured by its specific power (the reciprocal of the coefficient of performance). The specific power specifies how many watts are consumed by the refrigeration system for every watt of heat removed. The theoretical limit for specific power, given by the Carnot limit, is $\frac{T_H - T_C}{T_C}$. For liquid helium temperatures, the Carnot limit demands that at least $74$\,Watts of refrigeration power are required to remove every watt of heat produced on-chip. State-of-the-art systems have reached specific powers as low $250$\,W/W. Auspiciously, the most efficient refrigeration systems also tend to have the highest heat loads. Heat loads as high as 10\,kW at 4.4K have already been demonstrated by commercially available systems. Note that throughout this paper, we assume a more conservative specific power of $1000$\,W/W, representative of the smaller scale cryogenic systems used in most laboratories today. Overall, it does not appear that cryogenic capability will be an insurmountable obstacle towards large-scale superconducting neural systems.

Heat leaks are another important consideration for superconducting systems. There are various mechanisms by which heat from the ambient environment can enter the cryogenic chamber, including conduction through structural components, thermal radiation, and leakage through I/O interconnects. This unwanted thermal power adds to the burden of the cryogenic system. In reference \cite{holmes2013energy}, it is estimated that heat leaks will consume about 10\% of the refrigeration power budget for a digital superconducting von-Neumann system. While this adds a standby power dissipation that would otherwise not exist in a superconducting system (aside from supporting semiconductor electronics for I/O, power supplies, etc.), it is not particularly important for the order of magnitude estimations desired in this paper.

\subsubsection{Power Limitations}
Modern supercomputers typically consume megawatts of power. Tianhe 2, for instance, requires 17.8\,MW for operation (and another 6.4\,MW for cooling) \cite{tolpygo2016superconductor}. If we thus assume a total power budget on the order of 10 MW, we can analyze the trade-off between average firing rate and the number of neurons. We assume 1\,fJ of optical energy is required to at each synapse and plot the maximum average frequency spiking frequency for several different optical link efficiencies in figure \ref{fig:freq_size}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{Pow.pdf}
    \caption{Tradeoff between size and average spiking frequency for a population of optoelectronic neurons with a power budget of 10 MW. Fan-out is $10^3$ and the optical energy needed at each synapse is assumed to be 1 fJ. This likely would correspond to the limits of either superconductor or semiconductor neurons.}
    \label{fig:freq_size}
\end{figure}

Power does not appear to be a limiting factor in achieving brain-scale and brain-speed optoelectronic networks. If the power resources of modern supercomputers were dedicated to a brain-scale optoelectronic neuromorphic system, average spiking rates on the order of 10 kHz appear feasible even with relatively inefficient optical links. Such a system, if designed to cultivate similar activity as the human brain, would enable brain-scale simulation with time accelerated by four orders of magnitude.

Another factor to consider is power density. There is a maximum power density that can be handled by heat removal systems for both the semiconducting and superconducting case. In the semiconductor case, high-performance computing routinely generates power densities of hundreds of watts per square centimeter \cite{tolpygo2016superconductor}. A theoretical limit of around 1\,kW/cm$^2$ is postulated in ref \cite{zhirnov2003limits}. In contrast, superconducting systems will be required to operate at significantly lower power densities. Roughly 1 W/cm$^2$ is a conservative limit on the on-chip power density that can be cooled with liquid helium \cite{tolpygo2016superconductor}. Interestingly, superconducting optical links appear to be capable of dissipating about three orders of magnitude less energy per bit, approximately cancelling out the limited power density requirements of superconducting systems in comparison with semiconductors. In practice, it might well be the case that mature, sophisticated synapses and neurons will occupy so much area that these power density limitations will be of no consequence. For instance, even with link efficiencies of $1 \times 10^{-4}$, a synapse would require a lateral dimension of less than 30\,$\mu$m for power density considerations to limit spiking to less than 1 GHz. However, it should also be noted that optoelectronic systems will have extremely nonuniform power dissipation across the chip/wafer, with most of the power being dissipated at the light sources. A more in-depth analysis is required to see if heat removal will be an issue near the light-sources in particular. Concerns about local heating may be assuaged with layouts that sufficiently separate thermally sensitive devices from the light sources.

%\section{Thermodynamic Considerations}
%\subsection{Heat Management}
%\subsection{Cryogenics}
%\section{Neuronal Computation}
%\subsection{Aggregating Inputs}
%\subsection{Digital Neurons}
%\subsection{Sub-Threshold Transistor Circuits}
%\subsection{Josephson Neurons}
%\section{Neuronal Pool}

%\section{Economic Viability}
%\subsection{Wafer Fabrication Costs}
%\subsection{Power Costs}
%\subsection{Cryostat Manufacturing Costs}

\section{\label{sec:conclusion}Conclusion}

The prospects of neuromorphic systems at the scale of the brain and beyond are tantalizing. The fan-out capability of optical communication, coupled with the computational power of electronic circuitry makes optoelectronic systems a promising framework for realizing these high ambitions. However, a number of hardware breakthroughs need to occur to make this vision a reality. Efficient, densely integrated light sources, waveguide-integrated detectors, local memory devices, and capable neuronal circuitry must all be consolidated onto a single platform. Fortunately, excellent prior work has produced a variety of options on all of these fronts. The great challenge is now to find the optimal subset of technologies that can be practically interfaced together while maximizing performance. Both semiconducting and superconducting technologies appear to hold the potential of supplying all the requisite devices for the application, and might even operate with surprisingly similar performance. However, the paths toward achieving brain-scale systems are dramatically different for the two platforms, and we have attempted to outline the main advantages and challenges for each one. 

Semiconductor optoelectronic neural systems can claim advantages in speed, fabrication infrastructure, and room-temperature operation over their superconducting counterparts. Spike rates in excess of 10 GHz are feasible, but static power dissipation is concerning for networks that are desired to operate at lower activity levels. Semiconductor receivers can potentially operate with extremely low energies per spiking event (sub femto-joule), making them a worthy competitor of superconducting single photon detectors. However, these low energy receivers require significant optical power out of integrated light sources. To achieve biological-scale fan-out, either repeatering schemes (costing area and yield) or additional gain stages (costing power) will need to be included. In terms of neuronal computation, semiconductor neurons have already demonstrated impressive functionality and low power operation that should be capable of integration with optical communication infrastructure. Memory is a concern, but a variety of non-volatile memory solutions have seen extensive investigation. Still, a large-scale implementation of integrated memory with local plasticity update circuits is yet to be demonstrated. Lastly, 3D integration of transistors, photodetectors, and memory would be an extremely challenging endeveour, given the necessity of high-temperature processing. The fabrication processes for mature semiconductor neural systems may prove to be extremely complicated and heterogeneous, perhaps requiring different materials for sources, detectors, and memories.

Superconducting optoelectronic neural systems do not benefit as greatly from the maturity of the semiconductor industry, but provide several intriguing attributes. Namely, SNSPD receivers place nearly the theoretical minimum burden on integrated light sources. This is coupled with the known improvement in efficiency for light sources operating at cryogenic temperatures. These two factors make the large-scale integration of light sources appear far more achievable than in the semiconductor case - perhaps even opening the door to silicon as an active optical material. Driving these light sources with superconducting electronics, however, has yet to demonstrated in a practical manner and is a major open question. Superconducting neuronal circuits appear just as capable of complex computation as their CMOS counterparts, but will likely need to be designed with serial biasing in order to scale. Additionally, the speed advantages present in superconducting electronics will be negated by the response time of SNSPDs, making operation above 1 GHz unlikely. Memory seems to be a strength, however, as superconductivity provides new avenues of storing synaptic weights and loop memory in particular may be capable of implementing plasticity mechanisms that operate with only the signals produced through normal network activity. Finally, 3D integration appears more promising in the superconductor platform and cooling systems are unlikely to be a limiting factor. If all of these issues can be resolved, superconducting optoelectronic systems may actually require simpler processes than semiconductor approach, as the material ecosystem could potentially be relatively simple. Of course, superconducting foundries are far less developed than their semiconductor counterparts, which may negate these advantages in the near-term.

Finally, we would be remiss to paint the quest for neuromorphic supercomputing as only a question of hardware. The biological details of the inner workings of the brain remain an enigma of a magnitude eclipsed only by the question of how those inner workings could possibly give rise to phenomena of the likes of cognition and consciousness. Breakthroughs in neuroscience and algorithmic development (real spiking algorithms, not crude impersonations of back-propagation) will be required for the discussed hardware platforms to ever have any practical use. This summons the long-nagging chicken-or-the-egg question of whether we need hardware for algorithms or algorithms for hardware in neuromorphic computing. Fortunately, when you assume a 1000 year perspective on hardware, you might as well get started now, since you're going to need all the time you can get.


\bibliographystyle{unsrt}
\bibliography{bib}
\end{document}
